{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tensor Basics\n",
    "This notebook explains different kinds of tensors in PyTorch — scalars, vectors, matrices, and higher-dimensional tensors — along with basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar\n",
    "A scalar is a single number (0-dimensional tensor).\n",
    "1. Structure.ndim - gives the number of dimension the structure have.\n",
    "2. Structure.item - converts the type of the structure back to the python int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "0\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "scalar=torch.tensor(7)\n",
    "print(scalar)\n",
    "print(scalar.ndim) #no. of dimention does scalar have\n",
    "print(scalar.item()) #Get tensor back as python int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector (1D tensor)\n",
    "1. Structure.shape - gives the structure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 9])\n",
      "1\n",
      "tensor(9)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "vector=torch.tensor([6,9])\n",
    "print(vector)\n",
    "print(vector.ndim)\n",
    "print(vector[1])\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix (2D tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  8],\n",
      "        [ 9, 10]])\n",
      "2\n",
      "tensor([ 9, 10])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "matrix=torch.tensor([[7,8],\n",
    "                     [9,10]])\n",
    "print(matrix)\n",
    "print(matrix.ndim)\n",
    "print(matrix[1])\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor (3D example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 3,  6,  9],\n",
      "         [10, 11, 12]]])\n",
      "3\n",
      "torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.tensor([[[1,2,3],\n",
    "                      [3,6,9],\n",
    "                      [10,11,12]]])\n",
    "print(tensor)\n",
    "print(tensor.ndim)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tensor\n",
    "Creating a Random Tensor of size --> rows=3 and columns=4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7393, 0.6600, 0.9801, 0.9794],\n",
      "        [0.0938, 0.1872, 0.2850, 0.2504],\n",
      "        [0.8193, 0.8328, 0.4975, 0.5142]])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "random_tensor=torch.rand(size=(3,4))\n",
    "print(random_tensor)\n",
    "print(random_tensor.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random tensor shaped like an image (Height, Width, Channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([224, 224, 3])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "random_image_size_tensor=torch.rand(size=(224,224,3))\n",
    "print(random_image_size_tensor.shape)\n",
    "print(random_image_size_tensor.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeros and Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "zeros=torch.zeros(size=(3,4))\n",
    "print(zeros)\n",
    "print(zeros*random_tensor)\n",
    "ones=torch.ones(size=(3,4))\n",
    "print(ones)\n",
    "print(ones.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ranges of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "tensor([  1,  51, 101, 151, 201, 251, 301, 351, 401, 451, 501, 551, 601, 651,\n",
      "        701, 751, 801, 851, 901, 951])\n"
     ]
    }
   ],
   "source": [
    "zero_to_nine=torch.arange(0,10)\n",
    "print(zero_to_nine)\n",
    "one_to_ten=torch.arange(1,11)\n",
    "print(one_to_ten)\n",
    "one_to_thousand_with_50steps=torch.arange(start=1, end=1000, step=50)\n",
    "print(one_to_thousand_with_50steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tensor-like another tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "one_to_ten=torch.zeros_like(input=one_to_ten)\n",
    "print(one_to_ten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Datatypes and Devices\n",
    "**NOTE:** tensor datatypes is one of the 3 big errors you'll run into with pytorch & deep learning \n",
    "1. tensor not right `datatype`\n",
    "2. tensor not right `shape`\n",
    "3. tensor not on the right `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.])\n",
      "torch.float32\n",
      "tensor([2., 4., 6.], device='cuda:0', dtype=torch.float16)\n",
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "float32_tensor=torch.tensor([3.0, 6.0, 9.0],\n",
    "                            dtype=None, #what data type is the tensor (default=float32)\n",
    "                            device=None, #what device is your tensor on (default=None=cpu)\n",
    "                            requires_grad=False) #whether or not to track gradiants with this tensor operation\n",
    "print(float32_tensor)\n",
    "print(float32_tensor.dtype)\n",
    "float16_tensor=torch.tensor([2.0, 4.0, 6.0],\n",
    "                            dtype=torch.float16,\n",
    "                            device=\"cuda\",\n",
    "                            requires_grad=False)\n",
    "print(float16_tensor)\n",
    "print(float16_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 6., 9.], dtype=torch.float16)\n",
      "tensor([ 9., 36., 81.])\n",
      "tensor([2, 3, 4], dtype=torch.int32)\n",
      "tensor([ 6., 18., 36.])\n"
     ]
    }
   ],
   "source": [
    "float16_tensor=float32_tensor.type(torch.float16)\n",
    "print(float16_tensor)\n",
    "print(float16_tensor*float32_tensor)\n",
    "int_32_tensor=torch.tensor([2,3,4], dtype=torch.int32)\n",
    "print(int_32_tensor)\n",
    "print(float32_tensor*int_32_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking tensor attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7468, 0.6562, 0.6914, 0.3129],\n",
      "        [0.5350, 0.2398, 0.3274, 0.7300],\n",
      "        [0.9353, 0.1742, 0.6144, 0.7001]])\n",
      "Datatype of tensor: torch.float32\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Device of tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "some_tensor=torch.rand(3, 4)\n",
    "print(some_tensor)\n",
    "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
    "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
    "print(f\"Device of tensor: {some_tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic tensor operations (Addition, Subtraction, Multiplication, Division)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([11, 12, 13])\n",
      "tensor([-9, -8, -7])\n",
      "tensor([10, 20, 30])\n",
      "tensor([0.1000, 0.2000, 0.3000])\n",
      "tensor([11, 12, 13])\n",
      "tensor([-9, -8, -7])\n",
      "tensor([10, 20, 30])\n",
      "tensor([0.1000, 0.2000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.tensor([1,2,3])\n",
    "print(tensor+10)\n",
    "print(tensor-10) #SLOW\n",
    "print(tensor*10)\n",
    "print(tensor/10)\n",
    "#OR\n",
    "print(torch.add(tensor, 10))\n",
    "print(torch.sub(tensor, 10)) #FAST\n",
    "print(torch.mul(tensor, 10))\n",
    "print(torch.div(tensor, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Multiplication\n",
    "**NOTE:** There are 2 types of matrix multiplication:\n",
    "1. Element wise multiplication.\n",
    "2. Matrix multiplication (dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n",
      "tensor(14)\n",
      "tensor(14)\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"*\", tensor)\n",
    "print(f\"Equals: {tensor*tensor}\")\n",
    "print(torch.matmul(tensor, tensor))\n",
    "#OR\n",
    "print(tensor @ tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34839e54",
   "metadata": {},
   "source": [
    "## 2 Rules\n",
    "There are `2 main` rules that performing matrix multiplication needs to satisfy:\n",
    "### 1. Inner Dimension Should Match.\n",
    "1. torch.Size([3, 2]) @ torch.Size([3, 4]) `won't work`.\n",
    "2. torch.Size([3, 2]) @ torch.Size([2, 5]) `will work`.\n",
    "3. torch.Size([2, 3]) @ torch.Size([3, 1]) `will work`.\n",
    "\n",
    "**NOTE:** In matrix `A*B != B*A.`\n",
    "### 2. The resulting matrix has the shape of the Outer Dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e295bf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7058, 0.7404, 0.7055, 0.3954, 1.2452, 1.2197],\n",
      "        [0.2623, 0.2910, 0.2012, 0.1084, 0.4846, 0.4396]])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "matrixmul=torch.matmul(torch.rand(2,3), torch.rand(3,6))\n",
    "print(matrixmul)\n",
    "print(matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e2e5f1",
   "metadata": {},
   "source": [
    "## Transpose Matrix\n",
    "To Fix our tensor shape issues, we can manipulate the shape of one of our tensors using a `TRANSPOSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1696b8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3776, 0.9899, 0.4285],\n",
      "        [0.9229, 0.0337, 0.3562]])\n",
      "tensor([[0.3776, 0.9229],\n",
      "        [0.9899, 0.0337],\n",
      "        [0.4285, 0.3562]])\n",
      "tensor([[0.9404, 0.6884, 0.0543],\n",
      "        [0.0053, 0.9747, 0.5288]])\n",
      "tensor([[0.9404, 0.0053],\n",
      "        [0.6884, 0.9747],\n",
      "        [0.0543, 0.5288]])\n",
      "tensor([[0.3600, 1.1595, 0.5086],\n",
      "        [0.9311, 0.7143, 0.0716],\n",
      "        [0.4049, 0.6422, 0.2116]])\n",
      "tensor([[1.0599, 1.1935],\n",
      "        [0.9104, 0.2261]])\n"
     ]
    }
   ],
   "source": [
    "A=torch.rand(2,3)\n",
    "B=torch.rand(2,3)\n",
    "# print(A @ B) will not work cuz the inner dimension are not same\n",
    "# we have to transpose any one of them\n",
    "print(A)\n",
    "print(A.T)\n",
    "print(B)\n",
    "print(B.T)\n",
    "print(A.T @ B)\n",
    "print(A @ B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations (min, max, mean, sum, argmin, argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100])\n",
      "Minimum: 10\n",
      "Maximum: 100\n",
      "Mean: 55.0\n",
      "Sum: 550\n"
     ]
    }
   ],
   "source": [
    "tensorA=torch.arange(10, 110, 10)\n",
    "print(tensorA)\n",
    "print(f\"Minimum: {torch.min(tensorA)}\")\n",
    "print(f\"Maximum: {torch.max(tensorA)}\")\n",
    "print(f\"Mean: {torch.mean(tensorA.type(torch.float32))}\")\n",
    "# mean can calculated only for floating numbers\n",
    "print(f\"Sum: {torch.sum(tensorA)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49eedf",
   "metadata": {},
   "source": [
    "## Positional min, max and mean\n",
    "##### Shows the `position` of the min or max value in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d9c95dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(9)\n",
      "tensor(0)\n",
      "tensor(9)\n",
      "tensor(10)\n",
      "tensor(100)\n"
     ]
    }
   ],
   "source": [
    "print(tensorA.argmin())\n",
    "print(tensorA.argmax())\n",
    "print(torch.argmin(tensorA))\n",
    "print(torch.argmax(tensorA))\n",
    "print(tensorA[0])\n",
    "print(tensorA[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping, Stacking, Squeezing, Unsqueezing, Permuting\n",
    "\n",
    "`Reshaping` - reshapes the input tensor to a defined shape\n",
    "\n",
    "`View` - return a view of an input tensor of certain shape but keep the same memory as the original tensor\n",
    "\n",
    "`Stacking` - combine multiple tensors on top of eachother (vstack) or side by side (hstack)\n",
    "\n",
    "`Squeezwe` - removes all one dimension from a tensor\n",
    "\n",
    "`Unsqueeze` - adds a one dimension to a target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]) torch.Size([10])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]]) torch.Size([1, 10])\n",
      "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]]) torch.Size([1, 10])\n",
      "tensor([[ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10]]) tensor([ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n",
      "tensor([[ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
      "        [ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10]])\n",
      "tensor([[ 5,  5,  5,  5],\n",
      "        [ 2,  2,  2,  2],\n",
      "        [ 3,  3,  3,  3],\n",
      "        [ 4,  4,  4,  4],\n",
      "        [ 5,  5,  5,  5],\n",
      "        [ 6,  6,  6,  6],\n",
      "        [ 7,  7,  7,  7],\n",
      "        [ 8,  8,  8,  8],\n",
      "        [ 9,  9,  9,  9],\n",
      "        [10, 10, 10, 10]])\n",
      "Previous tensor and its shape:  tensor([[ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10]]) torch.Size([1, 10])\n",
      "After removing extra dimensions from x_reshape:  tensor([ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10]) torch.Size([10])\n",
      "Previous tensor and its shape:  tensor([ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10]) torch.Size([10])\n",
      "After unsqueezing the squeesed tensor be like:  tensor([[ 5,  2,  3,  4,  5,  6,  7,  8,  9, 10]]) torch.Size([1, 10])\n",
      "After unsqueezing the squeesed tensor be like:  tensor([[ 5],\n",
      "        [ 2],\n",
      "        [ 3],\n",
      "        [ 4],\n",
      "        [ 5],\n",
      "        [ 6],\n",
      "        [ 7],\n",
      "        [ 8],\n",
      "        [ 9],\n",
      "        [10]]) torch.Size([10, 1])\n",
      "tensor([[[0.9562, 0.5981],\n",
      "         [0.3626, 0.7487],\n",
      "         [0.0632, 0.8466],\n",
      "         [0.5634, 0.2989]],\n",
      "\n",
      "        [[0.9965, 0.1492],\n",
      "         [0.9442, 0.4459],\n",
      "         [0.5577, 0.0271],\n",
      "         [0.2255, 0.3531]],\n",
      "\n",
      "        [[0.5830, 0.5352],\n",
      "         [0.6851, 0.1989],\n",
      "         [0.2676, 0.8626],\n",
      "         [0.7327, 0.1320]]]) torch.Size([3, 4, 2])\n",
      "tensor([[[0.9562, 0.3626, 0.0632, 0.5634],\n",
      "         [0.9965, 0.9442, 0.5577, 0.2255],\n",
      "         [0.5830, 0.6851, 0.2676, 0.7327]],\n",
      "\n",
      "        [[0.5981, 0.7487, 0.8466, 0.2989],\n",
      "         [0.1492, 0.4459, 0.0271, 0.3531],\n",
      "         [0.5352, 0.1989, 0.8626, 0.1320]]]) torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "#Reshaping\n",
    "x=torch.arange(1, 11)\n",
    "print(x, x.shape)\n",
    "x_reshaped=x.reshape(1,10) # only works when (1*10 = actual x size)\n",
    "print(x_reshaped, x_reshaped.shape)\n",
    "\n",
    "#Viewing\n",
    "z=x.view(1,10)\n",
    "print(z,z.shape) # changing z changes x(bcuz a view of tensor shares the same memory as the original input)\n",
    "z[:, 0]=5\n",
    "print(z,x)\n",
    "\n",
    "#Stacking\n",
    "x_stacked=torch.stack([x,x,x,x],dim=0) #hstack\n",
    "print(x_stacked)\n",
    "x_stacked=torch.stack([x,x,x,x],dim=1) #vstack\n",
    "print(x_stacked)\n",
    "\n",
    "#Squeezing\n",
    "print(f\"Previous tensor and its shape: \",x_reshaped,x_reshaped.shape)\n",
    "x_squeezed=x_reshaped.squeeze()\n",
    "print(f\"After removing extra dimensions from x_reshape: \",x_squeezed,x_squeezed.shape)\n",
    "\n",
    "#Unsqueeze\n",
    "print(f\"Previous tensor and its shape: \",x_squeezed,x_squeezed.shape)\n",
    "x_unsqueezed=x_squeezed.unsqueeze(dim=0) #need the dimension input to unsqeeze the tenor (adding the dimension)\n",
    "print(f\"After unsqueezing the squeesed tensor be like: \",x_unsqueezed,x_unsqueezed.shape)\n",
    "x_unsqueezed=x_squeezed.unsqueeze(dim=1)\n",
    "print(f\"After unsqueezing the squeesed tensor be like: \",x_unsqueezed,x_unsqueezed.shape)\n",
    "\n",
    "#Permute\n",
    "x_og=torch.rand(size=(3,4,2)) #[height, width, colorchannel] \n",
    "print(x_og,x_og.shape)\n",
    "x_permuted=x_og.permute(2,0,1) #shifting-->[height-->2, width-->0, colorchannel-->1]\n",
    "print(x_permuted,x_permuted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5bfca",
   "metadata": {},
   "source": [
    "## Indexing (selectinhgdata from tensors)\n",
    "indexing with Py Torch is similar to indexing with NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f8cc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]]) torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x= torch.arange(1,10).reshape(1,3,3)\n",
    "print(x,x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44f61bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "#lets index on our new tensors\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a203761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "#lets index on the middle bracket (dim=1)\n",
    "print(x[0][0])\n",
    "#OR\n",
    "print(x[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5dd029e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n",
      "tensor(1)\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "#lets index on the most inner bracket (last dimension)\n",
    "print(x[0,0,0])\n",
    "#OR\n",
    "print(x[0][0][0])\n",
    "print(x[0,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "badbb52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]]) torch.Size([1, 3, 3])\n",
      "tensor([[1, 2, 3]])\n",
      "tensor([[2, 5, 8]])\n",
      "tensor([5])\n",
      "tensor([1, 2, 3])\n",
      "tensor([9])\n",
      "tensor([[3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(x,x.shape)\n",
    "#you can also use \":\" to select \"all\" of a target dimension\n",
    "print(x[:,0])\n",
    "# get all values of 0th and 1st dimension but only index 1 of 2nd dimension\n",
    "print(x[:,:,1])\n",
    "# get all values of the 0 dimension but only the 1 index value of 1st and 2nd dimension\n",
    "print(x[:,1,1])\n",
    "# get index 0 of the 0th and 1st dimension and all values of 2nd dimension\n",
    "print(x[0,0,:])\n",
    "# index on x to return 9\n",
    "print(x[:,2,2])\n",
    "# index on x to return 3,6,9\n",
    "print(x[:,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb2ca9",
   "metadata": {},
   "source": [
    "## PyTorch tensors & NumPy\n",
    "\n",
    "Numpy is a popular scientific python numerical computing library.\n",
    "\n",
    "And because of this, PyTorch has functionality to intereact with it.\n",
    "\n",
    "* Data in NumPy, want in PyTorch tensor -> `torch.from_numpy(ndarray)`\n",
    "\n",
    "* PyTorch tensor -> Numpy -> `torch.tensor.numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cceabbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 4. 5. 6. 7.] tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# NumPy array from Tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "array=np.arange(1.0,8.0) #default dtype=float64\n",
    "tensor=torch.from_numpy(array) #warning: when converting from numpy -> pytorch, pytorch reflects numpy's deafult datatype of float64 unless specified otherwise\n",
    "print(array,tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c7d902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4. 5. 6. 7. 8.] tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#change the value of array, what will this do to 'tensor'?\n",
    "array=array+1\n",
    "print(array,tensor)\n",
    "# hence changing the value of numpy dosnt affect the values of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a6fac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1.]) [1. 1. 1. 1. 1. 1. 1.] float32\n"
     ]
    }
   ],
   "source": [
    "# tensor to Numpy array\n",
    "tensor=torch.ones(7) #default dtype=float32\n",
    "numpy_tensor=tensor.numpy()\n",
    "print(tensor, numpy_tensor, numpy_tensor.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4765aaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2., 2.]) [1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "#Change the tensor, what happens to 'numpy_tensor'?\n",
    "tensor=tensor+1\n",
    "print(tensor,numpy_tensor)\n",
    "# hence changing the value of tensor dosnt affect the values of numpy_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f6dedc",
   "metadata": {},
   "source": [
    "## Reproducblity (trying to take random out of random\n",
    "\n",
    "In short how a neural network learns:\n",
    "\n",
    "`start with random numbers -> tensor operation -> update random numbers to try and make them better representation of the data -> again -> again -> again -> again...`\n",
    "\n",
    "To REDUCE the randomness in neural networks and PyTorch comes the concept of a **random seed**\n",
    "\n",
    "Essentially what the random seed does is \"flavour\" the randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "703b8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0045, 0.6063, 0.3834, 0.2038],\n",
      "        [0.6769, 0.0197, 0.3728, 0.0821],\n",
      "        [0.5852, 0.5693, 0.5256, 0.7238]])\n",
      "tensor([[0.8747, 0.4385, 0.8609, 0.5936],\n",
      "        [0.8376, 0.1903, 0.0400, 0.2775],\n",
      "        [0.1994, 0.4720, 0.7899, 0.7149]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "random_tensorA=torch.rand(3,4)\n",
    "random_tensorB=torch.rand(3,4)\n",
    "print(random_tensorA)\n",
    "print(random_tensorB)\n",
    "print(random_tensorA==random_tensorB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "352d989f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "#lets make some randombut reproducible tensors\n",
    "import torch\n",
    "#set the random seed\n",
    "RANDOM_SEED=42\n",
    "torch.manual_seed(RANDOM_SEED) #its works only 1 number of block when using in notebook\n",
    "random_tensorC= torch.rand(3,4)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensorD= torch.rand(3,4)\n",
    "print(random_tensorC)\n",
    "print(random_tensorD)\n",
    "print(random_tensorC==random_tensorD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d909b",
   "metadata": {},
   "source": [
    "## Running tensors and PyTorch objects on GPUs (and making faster computataions)\n",
    "\n",
    "GPUs= faster computation on numbers, thanks to CUDA + NVIDIA hardware + PyTorch working BTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83480e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 11 20:07:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.88                 Driver Version: 576.88         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3050 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   45C    P3              4W /   35W |      93MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A           21172      C   ...s\\Python\\Python311\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01f156fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for GPU access with PyTorch\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b76705b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setup device agnostic code\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a872a347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#COUNT NUMBER OF GPUs\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e10db4f",
   "metadata": {},
   "source": [
    "## Putting tensors (and/models) on the GPU\n",
    "the reason we want our tensor/models on the GPU is bcuz using GPU results in faster computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aabc8caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n",
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#create a tensor (default on the CPU)\n",
    "tensor =torch.tensor([1,2,3])\n",
    "\n",
    "#tensor not on GPU\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "#Move tensor to GPU (if available)\n",
    "tensor_on_gpu=tensor.to(device)\n",
    "print(tensor_on_gpu)\n",
    "# There is only 1 gpu in this device so the index of the gpu is '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a7341",
   "metadata": {},
   "source": [
    "### Numpy only works with cpu\n",
    "Therefore we have to move tensors back to the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b2d26c83",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#If tensors is on GPU, cant transform it to NumPy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtensor_on_gpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#If tensors is on GPU, cant transform it to NumPy\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71dce561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] cpu\n",
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# To fix the GPU tensor with NumPy issue, we can first set the tensor to CPU\n",
    "tensor_back_on_CPU=tensor_on_gpu.cpu().numpy()\n",
    "print(tensor_back_on_CPU, tensor_back_on_CPU.device)\n",
    "print(tensor_on_gpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
